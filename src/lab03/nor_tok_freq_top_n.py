import re


# NORMALIZE
def normalize(text: str, *, casefold: bool = True, yo2e: bool = True):
    text = re.sub(r"[\x00-\x1F\x7F\u200B\uFEFF]", " ", text).casefold()
    while "  " in text:
        text = text.replace("  ", " ")
    if casefold == True:
        text = text.casefold()
    if yo2e == True:
        text = text.replace("–Å", "–ï").replace("—ë", "–µ")
    return text.strip()


normalize_case_text = [
    "–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t",
    "—ë–∂–∏–∫, –Å–ª–∫–∞",
    "Hello\r\nWorld",
    "  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  ",
]
for case_text in normalize_case_text:
    print(normalize(case_text))
print("-" * 20)


# TOKENIZE
def tokenize(text: str):
    return re.findall(r"\w+(?:-\w+)*", text)


tokenize_case_text = [
    "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä",
    "hello,      world!!!",
    "–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ",
    "2025 –≥–æ–¥",
    "emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ",
]
for case_text in tokenize_case_text:
    print(tokenize(case_text))
print("-" * 20)


# COUNT_FREQ
def count_freq(tokens: list[str]):
    token_keys_all = sorted(set(tokens))
    return sorted(
        {token_key: tokens.count(token_key) for token_key in token_keys_all}.items(),
        key=lambda item: (-item[1], item[0]),
    )


count_freq_case_tokens = [
    ["a", "b", "a", "c", "b", "a"],
    ["bb", "aa", "bb", "aa", "cc"],
]
for case_tokens in count_freq_case_tokens:
    print(count_freq(case_tokens))
print("-" * 20)


# TOP_N
def top_n(freq: dict[str, int], n: int = 5):
    return sorted(freq.items(), key=lambda item: (-item[1], item[0]))[:n]


top_n_case_freq = [{"b": 2, "a": 3, "c": 1}, {"bb": 2, "cc": 1, "aa": 2}]
for case_freq in top_n_case_freq:
    print(top_n(case_freq))
print("-" * 20)
